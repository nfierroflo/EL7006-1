{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-787c2a5d-4a0a-456f-92c5-226d7952e827",
    "colab_type": "text",
    "deepnote_cell_type": "markdown",
    "id": "Um1J0VRVN5b7"
   },
   "source": [
    "<h1><center> EL7006 - Tarea Nº4 <br />Generative Models:  Variational Autoencoders <br/> </center></h1>\n",
    "<h3><center>Profesor: Pablo Estévez <br />\n",
    "Profesor Auxiliar: Jhon Intriago <br />\n",
    "Ayudantes: Bastián Gamboa, Daniel Neira, Giovanni Castiglioni <br />\n",
    "<h3><center>Semestre: Primavera 2022 </center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-79a0b545-ecae-4985-a425-f58c3635bf2d",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<h1><center><u>Parte Teórica</u></center></h1>\n",
    "\n",
    "\n",
    "\n",
    "Como se vió en la clase de VAEs, la función de costo a optimizar está dada por:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{negative-ELBO}}   = \\mathbb{E}_{q(x)}[- \\mathbb{E}_{q(z|x)}[\\log p(x|z)] + D_{KL}(q(z|x)|| p(z)) ] \n",
    "$$\n",
    "\n",
    "Donde $D_{KL}(q(z|x)|| p(z)) = \\mathbb{E}_{q(z|x)}[\\log q(z|x) - \\log p(z)] $. En general las entropías y entropías cruzadas en el término $\\mathcal{L}_{\\text{negative-ELBO}}$ se obtienen de forma cerrada. Nos centraremos en el término regularizador $D_{KL}(q(z|x)|| p(z))$. \n",
    "\n",
    "Para obtener una función de pérdida cerrada en $D_{KL}(q(z|x)|| p(z))$ se realizan ciertas suposiciones. Las más relevantes son:\n",
    "\n",
    "* La suposición mean field del encoder $q(z|x) = \\prod_{j=1}^J q(z_j|x)$. Es decir no existe una correlación entre las variables latentes inferidas. $J$ el número de dimensiones.\n",
    "* Se asume que $q(z_j|x)$ distribuye normal.\n",
    "* Se asume una distribución prior $p(z) = \\mathcal{N}(0, I)$.\n",
    "\n",
    "Se obtendrá la función de costo de VAEs partiendo de un caso un poco más general, considerando $q(z|x) = \\mathcal{N}(\\mu_1, \\Sigma_1)$ y $p(z) = \\mathcal{N}(\\mu_2, \\Sigma_2)$ y posteriormente aplicando las suposiciones mencionadas se llegará a la fórmula típica usada en VAEs. Tomar $q(z|x)$ o $p(z)$ como distribuciones un poco más generales le dan mayor flexibilidad al modelo.\n",
    "\n",
    "Para esto, considere los siguientes pasos: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-ac27fe9d-b016-4a46-828c-c7198470858e",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "* Calcule $\\int q(z|x) \\log p(z)$. Como está trabajando con matrices le puede servir http://www2.imm.dtu.dk/pubdb/pubs/3274-full.html. Notar que debe obtener una solución matricial para esta entropía cruzada negativa.\n",
    "\n",
    "     <font size=1 color=red> <b>Escriba su demostración con todos los pasos aquí </b></font>     \n",
    "\n",
    "\n",
    "* Con el resultado anterior, que debiera estar en términos matriciales, asuma la suposición mean field tanto en $q(z|x)$ como $p(z)$ y llegue a \n",
    "$\n",
    "\\frac{1}{2}\\sum_{j=1}^J \\left( - \\log(2\\pi\\sigma_{2j}^2) - \\frac{\\sigma^2_{1j}}{\\sigma^2_{2j}} - \\frac{(\\mu_{1j} - \\mu_{2j})^2}{\\sigma_{2j}^2} \\right)\n",
    "$. Donde para $\\mu$ y $\\sigma^2$ el primer índice se refiere a la distribución y el segundo a las dimensiones. La demostración para el caso multivariado utilizando mean field se puede encontrar en https://arxiv.org/abs/1611.05148. No se utiliza esta demostración, se incluye por completitud.\n",
    "\n",
    "    <font size=1 color=red> <b>Escriba su demostración aquí </b></font> \n",
    "    \n",
    "    \n",
    "* Utilizando la expresión anterior, tomando prior $p(z) = \\mathcal{N}(0,I)$ y utilizando la expresión $D_{KL}(q(z|x)|| p(z)) = \\mathbb{E}_{q(z|x)}[\\log q(z|x) - \\log p(z)] $ , llegue a la fórmula cerrada de esta divergencia utilizada en variational autoencoders dada por $\\frac{1}{2}\\sum_{j = 1}^J \\left( \\mu(X) + \\sigma^2(x) - \\log \\sigma^2(x) - 1  \\right)$.\n",
    "\n",
    "    <font size=1 color=red> <b>Escriba su demostración aquí </b></font> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-74209d70-bdfc-4cdc-b318-689438387b70",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<h1><center><u>Parte Práctica </u></center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-f3402fa2-6b31-47ca-8cd2-f880be905bf7",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "En este parte, usted debe implementar y entrenar un modelo de VAEs. Además deberá comparar el desempeño de estos modelos según la métrica de evaluación solicitada. \n",
    "\n",
    "\n",
    "<h2>Preliminares: Google Colab</h2>\n",
    "\n",
    "Para evitar limitaciones de hardware, ejecute este notebook en Google Colab con GPU. Para ello siga estos pasos:\n",
    "\n",
    "- Suba este notebook a su cuenta de Google Drive.\n",
    "- Abra el notebook. Se hará automáticamente en Google Colab.\n",
    "- En el menú \"Entorno de ejecución\", seleccione \"Cambiar tipo de entorno de ejecución\" y en \"Acelerador por hardware\" seleccione GPU.\n",
    "\n",
    "<h2>Preliminares: PyTorch</h2>\n",
    "\n",
    "Por simplicidad, en esta tarea se utilizará PyTorch. Se entrega una estructura general del código a implementar, con el objetivo de no requerir conocimientos profundos del framework utilizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-5cc5d506-145d-429d-923e-c826eb30321d",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<h2> Funcionamiento VAE  </h2>\n",
    "\n",
    "A continuación se recuerda cómo funciona un Variational Autoencoder.\n",
    "\n",
    "1. Se samplea de la distribución empírica $q(x)$. (Esto sería el data loader)\n",
    "\n",
    "2. A través del encoder se obtienen los $\\mu$ y los $\\log \\sigma^2$. En general se parametriza $\\log \\sigma^2$ en vez de $\\sigma$, pues la salida generalmente se toma como lineal y basta tomar la exponencial de $\\log \\sigma^2$ para obtener $\\sigma^2$ y así asegurar que este sea positivo.\n",
    "\n",
    "3. Se samplea ruido proveniente de una distribución normal $\\epsilon \\sim \\mathcal{N}(0, I)$\n",
    "4. Se utiliza el truco de [reparametrización](https://arxiv.org/abs/1312.6114) para samplear de $q(z|x)$ y además optimizar sus parametros (los parámetros del encoder). Esto es, el $\\tilde{z}$ sampleado de $q(z|x)$ se puede escribir como $\\tilde{z} = \\mu(x) + \\sigma(x) \\odot \\epsilon $. Este paso en general se utiliza para optimizar el encoder mediante la función de pérdida asociada a la verosimilitud. La divergencia no toma en cuenta cuantos sampleos se hagan ya que se calcula de forma cerrada. En caso que no se calculase en forma cerrada, importaría el número de sampleos de $q(z|x)$. Para la tarea sólo debe samplear un $z(\\tilde{x}_i)$ por cada $x_i \\sim q(x) $. Esto es lo que se conoce como los sampleos de Monte-Carlo.\n",
    "5. Se calcula la verosomilitud negativa y se minimiza. Se calcula la divergencia de la parte teórica y se minimiza.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00006-d41312b9-8146-4770-afce-ba48f9afd1a0",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<h2> Deep Convolutional VAE  </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-b77d605f-089b-4780-b91a-c33aa9783870",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "En esta sección, usted deberá implementar una arquitectura muy similar a la pregunta de GANs en la que se utiliza la [DCGAN](https://arxiv.org/pdf/1511.06434v1.pdf). Se utilizará el mismo generador. El encoder utilizado será casi completamente similar al discriminador de la DCGAN.\n",
    "\n",
    "Alguna de las características de la DCGAN son: \n",
    "*   Usa convoluciones sin capas de pooling\n",
    "*   Usa BatchNormalization tanto en el Generador como en el Discriminador (En este caso no se tiene un Discriminador pero se utilizará un Encoder con las mismas características).\n",
    "*   No usa capas <i> Fully Connected </i>\n",
    "*   Usa activaciones ReLU en el Generador, salvo en la última capa donde usa Tanh. En este caso en vez de Tanh debe tener una salida lineal.\n",
    "*   Usa activaciones LeakyReLU en el Discriminador (en este caso Encoder), salvo en la última capa donde no usa activación\n",
    "\n",
    "A continuación se importan los paquetes a utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00008-47c7a118-b70f-4ea0-b2c2-684ade532ebd",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0); # Set for testing purposes, please do not change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00009-2aaa7dc1-b897-47fd-a572-1850ffb84bf6",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# If you have multiple devices and just want to use one, \n",
    "# then uncomment the following two lines:\n",
    "# i_device = 1\n",
    "# device = 'cuda:{0}'.format(i_device)\n",
    "# Otherwise, uncomment the following line:\n",
    "# device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-74e9977b-da5c-43bf-8c9b-c25e73865a54",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<h3> Bloque del Generador </h3>\n",
    "\n",
    "Para comenzar, deberá implementar los bloques que componen el Generador.\n",
    "Debido a que la función de activación varía según la capa, se deberá considerar esto al momento de la creación del bloque.\n",
    "\n",
    "La estructura de cada bloque deberá ser la siguiente: \n",
    "*   Una capa deconvolucional, utilizando los parámetros dados\n",
    "*   Una capa de batchnorm, salvo en la última capa\n",
    "*   Una activación ReLU después de cada batchnorm\n",
    "*   Una activacion lineal, sólo para la última capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00011-01e51a74-f7a2-4b06-a088-2b27be07025c",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "def make_gen_block(input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
    "    if not final_layer:\n",
    "        return nn.Sequential(\n",
    "        ### YOUR CODE HERE ###\n",
    "        )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "        ### YOUR CODE HERE ###\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00012-8ff74564-9f15-44a1-be39-feb09dc2bb5d",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "def get_noise(n_samples, z_dim, device='cpu'):\n",
    "    return torch.randn(n_samples, z_dim, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-7daf2f51-9bef-41eb-a21f-5a4439e8c224",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<h2> Generador</h2>\n",
    "\n",
    "Ahora construya un Generador formado por 4 bloques, con las siguientes especificaciones:\n",
    "\n",
    "    1.-block: z_dim          -> hidden_dim * 4 , kernel_size=3, stride=2\n",
    "    2.-block: hidden_dim * 4 -> hidden_dim * 2 , kernel_size=4, stride=1\n",
    "    3.-block: hidden_dim * 2 -> hidden_dim     , kernel_size=3, stride=2\n",
    "    4.-block: hidden_dim     -> im_chan        , kernel_size=4, stride=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00014-a3885911-65c2-4f32-a743-032359395d02",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=10, im_chan=1, hidden_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.gen = nn.Sequential(\n",
    "        ### YOUR CODE HERE ###\n",
    "        )\n",
    "\n",
    "    def unsqueeze_noise(self, noise):\n",
    "        return noise.view(len(noise), self.z_dim, 1, 1)\n",
    "\n",
    "    def forward(self, noise):\n",
    "        x = self.unsqueeze_noise(noise)\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00015-3d0f2c43-1a7a-49a0-94e5-b4ad01c47ff5",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<h3> Bloque del Encoder</h3>\n",
    "\n",
    "Procederemos a implementar un bloque para el Encoder.\n",
    "La estructura de cada bloque deberá ser la siguiente: \n",
    "\n",
    "*    Una capa convolucional, utilizando los parámetros dados\n",
    "*    Una capa de batchnorm, excepto por la última capa\n",
    "*    Una activacion LeakyReLU de pendiente 0.2, sólo después de las capas de batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00016-1b27ff81-c4d5-480b-93ad-47df155d2f97",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "def make_enc_block(input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):     \n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "            ### YOUR CODE HERE ###\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "            ### YOUR CODE HERE ###\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00017-a7e94622-4874-4c4f-aa3e-3f7b000ee525",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<h2> Encoder </h2>\n",
    "\n",
    "Ahora construya un Encoder formado por 3 bloques, con las siguientes especificaciones:\n",
    "\n",
    "    1.-block: im_chan        -> hidden_dim     , kernel_size=4, stride=2\n",
    "    2.-block: hidden_dim     -> hidden_dim * 2 , kernel_size=4, stride=2\n",
    "    3.-block: hidden_dim * 2 -> 2*z_dim        , kernel_size=4, stride=2\n",
    "    \n",
    "Además tiene que definir la función split_output, que separa la predicción del encoder en los $\\mu$ y $\\log \\sigma^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00018-3a2ed143-dbf3-4bcd-bc51-df00756dc5f3",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim = 2, im_chan=1, hidden_dim=16):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "        ### YOUR CODE HERE ###\n",
    "        )\n",
    "        \n",
    "    def split_output(self, features):\n",
    "        ### YOUR CODE HERE ###\n",
    "        \n",
    "    def forward(self, image):\n",
    "        enc_pred = self.enc(image)\n",
    "        return self.split_output(enc_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00019-0e09d951-efc9-462a-b480-598e7d8c3f3a",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "A continuación se definen los parámetros de entrenamiento a utlizar, y una función de utilidad para la inicialización de los pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00020-d4c68b74-7dff-4ead-9186-5b3f79f6ccf2",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# Setting the training parameters\n",
    "z_dim = 2\n",
    "display_step = 500\n",
    "batch_size = 128\n",
    "lr = 0.0002\n",
    "beta_1 = 0.5 \n",
    "beta_2 = 0.999\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    MNIST('.', download=True, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00021-fa83e214-c35a-4a34-94ff-4990aaca52db",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "gen = Generator(z_dim).to(device)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
    "enc = Encoder().to(device) \n",
    "enc_opt = torch.optim.Adam(enc.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "gen = gen.apply(weights_init)\n",
    "enc = enc.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00022-0ebd7ab6-87e4-4f12-b9c9-417a06ac3cab",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<h3> Funciones de Loss</h3>\n",
    "\n",
    "Implemente los métodos `get_neg_likelihood_loss` y `get_divergence_loss`. La función `get_neg_likelihood_loss` calcula la verosimilitud negativa en el espacio observable. Esta la debe calcular como un MSE entre la reconstrucción y el dato original como se vio en clases. La función `get_divergence_loss` es la divergencia vista en la parte teórica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00023-e8234fc2-227d-4a97-9c06-7ef90b1a1696",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "def get_neg_likelihood_loss(x, x_rec):\n",
    "    ### YOUR CODE HERE ###\n",
    "    neg_lik_loss = None\n",
    "    return neg_lik_loss\n",
    "\n",
    "def get_divergence_loss(mu, logvar):\n",
    "    ### YOUR CODE HERE ###\n",
    "    divergence_loss = None\n",
    "    return divergence_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00024-41e8f640-6c14-4272-b0d3-5bde24e41189",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<h2> Entrenamiento VAE </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00025-d8212c90-4a5a-45c9-b355-b301ac303c0f",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Muchas veces en la práctica se usa algo conocido como [$\\beta$-VAE](https://openreview.net/forum?id=Sy2fzU9gl) cuya función de costo es muy similar a la función de VAE. La única diferencia es un ponderador en la divergencia como se muestra a continuación:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{negative-ELBO}-\\beta-\\text{VAE}}   = \\mathbb{E}_{q(x)}[- \\mathbb{E}_{q(z|x)}[\\log p(x|z)] + \\beta \\cdot D_{KL}(q(z|x)|| p(z)) ] \n",
    "$$\n",
    "\n",
    "A continuación defina una función de entrenamiento de VAE dejando $\\beta$ como parámetro libre. En su entrenamiento comente todos los puntos mencionados en  <b>Funcionamiento VAE</b>. Utilice todas las funciones creadas en las secciones anteriores. Puede guiarse por el loop de entrenamiento utilizado en la pregunta de GANs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00026-e09ab4de-b09f-45bd-85dc-bca450e21c21",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<h2> Plot 2-D </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00027-0146b5a6-751d-4dff-91a6-3659aee40803",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Utilizando la función anterior, varíe $\\beta$ tal que la distribución marginal $q(z)$ sea aproximadamente una gaussiana. Para mostrar esto, genere tres plots 2D del espacio latente; uno con $\\beta$ bajo, otro con un valor medio y otro con valor alto. ¿Qué ocurre para cada valor de $\\beta$?"
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "a0965124-fe9e-409f-a127-c4654b9af074",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
