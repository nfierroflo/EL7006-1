{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Um1J0VRVN5b7"
   },
   "source": [
    "<h1> <center> EL7006 - Tarea Nº1 <br /> Redes Recurrentes y LSTM  </center> </h1>\n",
    "<h3> <center>Profesor: Pablo Estévez <br />\n",
    "Profesor Auxiliar: Jhon Intriago <br />\n",
    "Ayudantes: Bastián Gamboa, Daniel Neira, Giovanni Castiglioni <br />\n",
    "<h3><center>Semestre: Primavera 2023 </center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vl6c4hTGN5b-"
   },
   "source": [
    "En esta tarea deberán contestar preguntas e implementar código relacionado a las redes neuronales recurrentes, en particular a las tipo Long Short-Term Memory (LSTM). Se recomienda leer el capítulo 10 de [Deep Learning Book](https://www.deeplearningbook.org/).\n",
    "\n",
    "Para entregar su tarea, suba a u-cursos un archivo zip que contenga los notebooks de los problemas 1 y 2, desarrollados con sus respuestas e implementaciones, junto con cualquier archivo adicional necesario para ejecutarlos (excluyendo la base de datos). Sea claro en sus respuestas, y comente adecuadamente su código. No es necesario redactar un informe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><u>Problema 1: Teoría de redes recurrentes</u></center></h1>\n",
    "\n",
    "En este problema, usted deberá responder una serie de preguntas teóricas con respecto a las redes neuronales recurrentes.\n",
    "Utilice la siguiente notación para construir los grafos. La expresión $H = \\max(0, XW + b)$ corresponde al siguiente grafo:\n",
    "\n",
    "<img src=\"images/graph1.png\" alt=\"Drawing\" style=\"width: 220px;\"/>\n",
    "\n",
    "Las variables son representadas con circulos, y las operaciones aplicadas a cada variable con flechas. Retrasos temporales son representados con un cuadrado negro sobre las operaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Dada las siguientes expresiones para una capa de red neuronal recurrente:\n",
    "\n",
    "$$ a^{(t)} = b+Wh^{(t-1)}+Ux^{(t)}$$\n",
    "$$h^{(t)}=\\tanh(a^{(t)})$$\n",
    "$$ o^{(t)} = c + Vh^{(t)} $$\n",
    "$$\\hat{y}^{(t)} = \\text{softmax}(o^{(t)}) $$\n",
    "$$ \\mathrm{Loss}^{(t)} = L(y^{(t)}, \\hat{y}^{(t)}) $$\n",
    "\n",
    "Dibuje el grafo que describe las operaciones y el grafo desenrollado para 3 entradas consecutivas en los tiempos $t$, $t+1$, y $t+2$.\n",
    "\n",
    "\n",
    "Respuesta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente imagen se muestra el grafo que describe las operaciones dadas, se realizan simplificaciones para reducir la cantidad de nodos y hacerlo más comprensible. En el grafo desenrollado se omiten los sesgos b y c y las multiplicaciones se muestran en las flechas para no sobrecargarlo.\n",
    "<img src=\"images/respuesta1.png\" alt=\"Drawing\" style=\"width: 1080px;\"/>\n",
    "\n",
    "Las variables son representadas con circulos, y las operaciones aplicadas a cada variable con flechas. Retrasos temporales son representados con un cuadrado negro sobre las operaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) En el modelo anterior, las matrices $W$, $U$ y $V$ no varían en función del tiempo. Explique las ventajas de usar los parámetros de esta red neuronal recurrente invariantes al tiempo.\n",
    "\n",
    "Respuesta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El uso de parámetros de tiempo invariables en redes neuronales recurrentes (RNN) proporciona ventajas significativas. Estos parámetros compartidos, como $W$, $U$ y $V$, optimizan la eficiencia de los parámetros, promueven representaciones estables y consistentes en diferentes pasos de tiempo, mejoran el proceso de entrenamiento al facilitar la convergencia, reducen el costo computacional al realizar operaciones más eficientes, permiten una mejor generalización a secuencias de diferentes longitudes, simplifican la arquitectura de la red y brindan una mayor interpretabilidad, todo lo cual contribuye a la efectividad y robustez de las RNN en aplicaciones de modelado de secuencias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Considere el siguiente grafo:\n",
    "\n",
    "<img src=\"images/graph2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Escriba la salida $\\hat{y}$ en función de $x$, $W_{i}$, $f_{i}(\\ )$ para $i=1,2,3$. Luego calcule el gradiente de la salida $\\hat{y}$ con respecto a $W_1$, dejando la expresión en función de las matrices $W_{i}$ y las derivadas $\\displaystyle \\frac{df_{i}}{dU_{i}}(U_{i})$. \n",
    "\n",
    "Generalice su resultado para $i = 1,2,\\ldots,N$. Si $f_{i}(\\ )$ son funciones sigmoides o tangentes hiperbólicas aplicadas elemento a elemento, ¿Qué ocurre con la norma del gradiente a medida que $N$ aumenta? ¿Qué ocurre con la norma del gradiente si los elementos de cada $W_i$ tienen distribución gaussiana estándar $\\mathcal{N}(0, 1)$?\n",
    "\n",
    "Hint: Estudie el rango de valores numéricos para las derivadas de la sigmoide y tangente hiperbólica.\n",
    "\n",
    "Respuesta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\\hat{y}=f_3(W_3 f_2(W_2 f_1(W_1 x)))$$\n",
    "\n",
    "Al utilizar regla de la cadena, el gradiente de $\\hat{y}$ con respecto a $W_1$ resulta :\n",
    "\n",
    "$$\\frac{d\\hat{y}}{dW_1}(W_1)=\\frac{df_3}{dU_3}(U_3)\\cdot\\frac{dU_3}{df_2}(f_2)\\cdot\\frac{df_2}{dU_2}(U_2)\\cdot\\frac{dU_2}{df_1}(f_1)\\cdot\\frac{df_1}{dU_1}(U_1)\\cdot\\frac{dU_1}{dW_1}(W_1)=\\frac{df_3}{dU_3}(U3)\\cdot W_3\\frac{df_2}{dU_2}(U_2)\\cdot W_2\\cdot\\frac{df_1}{dU_1}(U_1)\\cdot x$$\n",
    "\n",
    "Al generalizar el resultado para i=1,2,...,N. Resulta:\n",
    "$$\\frac{d\\hat{y}}{dW_1}(W_1)=\\frac{df_1}{dU_1}(U_1)\\cdot x\\cdot\\prod_{i=2}^{N}\\frac{df_i}{dU_i}(Ui)\\cdot W_i$$\n",
    "Si $f_i()$ son funciones sigmoides las normas de sus derivadas están acotadas de forma estricta por 1:\n",
    "$$\\lVert\\sigma'(x)\\rVert=\\lVert\\sigma(x)(1-\\sigma(x))\\rVert=\\lVert\\sigma\\rVert  \\lVert 1-\\sigma\\rVert <1$$\n",
    "Si $f_i()$ son funciones tangentes hiperbólicas las normas de sus derivadas están acotadas de forma estricta por 1 si $x$ es distinto de 0, es decir, es menor a 1 \"almost surely\" y el conjunto que produce derivada igual a 1 tiene medida 0:\n",
    "$$\\lVert tanh'(x)\\rVert=\\lVert 1-tanh^2(x)\\rVert<1\\;a.s.$$\n",
    "De esta forma en ambos caso se cumple para la derivada del gradiente:\n",
    "$$\\lVert \\frac{d\\hat{y}}{dW_1}(W_1)\\rVert=\\lVert\\frac{df_1}{dU_1}(U_1)\\cdot x\\rVert\\cdot\\prod_{i=2}^{N}\\lVert\\frac{df_i}{dU_i}(Ui)\\rVert\\lVert\\cdot W_i\\rVert< \\lVert x\\rVert\\cdot\\lVert\\varepsilon^{N}\\rVert\\cdot\\prod_{i=2}^{N}\\lVert W_i\\rVert,\\:\\varepsilon\\in [0,1)$$\n",
    "Sin perdida de generalidad. Con esto al aumentar N, la norma del gradiente tiende a 0, es decir se \"desvanece\".\n",
    "Si $W_i\\sim\\mathcal{N}(0, 1)$ entonces la esperanza (media) del producto de N pesos (variables aleatorias independientes e idénticamente distribuidas (i.i.d.)), cada una siguiendo una distribución normal estándar $N(0,1)$, es igual a 0. Además la varianza del producto es $O(1^{n})=O(1)$ con esto la tendencia de los pesos será a estar entre -1 y 1 haciendo así que la norma disminuya y tienda a 0 a medida que N aumenta, nuevamente estamos ante el \"desvanecimiento del gradiente\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Considere el siguiente grafo:\n",
    "\n",
    "<img src=\"images/graph3.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "Escriba  $h$ en función de $h_{t-1}$, $x_{t}$, $W$ y $V$. Luego calcule el gradiente $\\displaystyle \\frac{dh_{t}}{dW}$, y escriba la expresión en función de $\\displaystyle \\frac{dh_{t-n}}{dW}$ ($n$ pasos atrás en el tiempo). \n",
    "\n",
    "Suponga que $V$ admite una descomposición en valores propios $\\lambda_{i}$ ¿Qué ocurre con la norma del gradiente a medida que $n$ aumenta? Estudie los casos $|\\lambda_i|<1$ y $|\\lambda_i|>1$. Comente al menos una solución para cada caso.\n",
    "\n",
    "Respuesta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h=Wx_t+Vh_{t-1}$$\n",
    "$$\\frac{dh_t}{dW}(W)=x_t+V[\\frac{dh_{t-1}}{dW}(W)]=x_t+V(x_{t-1}+V\\frac{dh_{t-2}}{dW}(W))=x_t+V(x_{t-1}+V(x_{t-2}+V\\frac{dh_{t-3}}{dW}(W)))=x_t+Vx_{t-1}+V^2x_{t-2}+V^3\\frac{dh_{t-3}}{dW}(W)$$\n",
    "Extendiendo este proceso n veces se obtiene\n",
    "$$\\frac{dh_t}{dW}(W)=x_t+Vx_{t-1}+V^2x_{t-2}+\\ldots+V^n\\frac{dh_{t-n}}{dW}(W)$$\n",
    "\n",
    "Cuando $V$ tiene una descomposición en valores propios $\\lambda_i$, la norma del gradiente $\\frac{dh_t}{dW}(W)$ a medida que $n$ aumenta depende de los valores propios $\\lambda_i$ de $V$. Vamos a estudiar dos casos:\n",
    "\n",
    "1. **$|\\lambda_i| < 1$:** En este caso, todos los valores propios de $V$ son menores que 1 en magnitud. A medida que aumentamos $n$, los términos de la forma $V^n$ en la expansión del gradiente se vuelven cada vez más pequeños porque $|V^n|$ disminuye rápidamente a medida que $n$ aumenta. Esto significa que los términos anteriores, como $V^{1}$, $V^{2}$, etc., dominarán la contribución a la norma del gradiente.\n",
    "\n",
    "2. **$|\\lambda_i| > 1$:** En este caso, al menos uno de los valores propios de $V$ es mayor que 1 en magnitud. A medida que aumentamos $n$, los términos de la forma $V^n$ en la expansión del gradiente se vuelven cada vez más grandes porque $|V^n|$ aumenta rápidamente a medida que $n$ aumenta. Esto significa que los términos anteriores, como $V^{n-1}$, $V^{n-2}$, etc., serán dominados por la contribución de $V^n$. En resumen, cuando $|\\lambda_i| > 1$, la norma del gradiente aumentará a medida que $n$ aumenta, y el gradiente crecerá indefinidamente. Esto indica que las contribuciones de las entradas más antiguas en la secuencia se volverán más y más importantes a medida que avanzamos en el tiempo.\n",
    "\n",
    "Además de manera análoga a como se muestra en la bibliografía sugerida Consideremos qué pasa a medida que propagamos un vector gradiente hacía atrás en el tiempo (backward through time). Si empezamos con un vector gradiente $g$, despues de un paso de propagación tendremos el término $Vg$ después de n pasos tendremos el componente $V^ng$, al realizar el mismo proceso con una version perturbada de g, si partimos con $g+\\delta v$ con $v$ vector propio de $V$ después de n pasos tendremos el termino $V^n(g+\\delta v)$ Si $v$ es elegido como un autovector unitario de $V$ con valor propio $\\lambda$, entonces la multiplicación por el Jacobiano simplemente escala la diferencia en cada paso. Las dos ejecuciones de retropropagación están separadas por una distancia de $\\delta|\\lambda|^n$. Cuando $v$ corresponde al valor más grande de $|\\lambda|$, esta perturbación logra la máxima separación posible de una perturbación inicial de tamaño $\\delta$. Cuando $|\\lambda| > 1$, el tamaño de la desviación $\\delta|\\lambda|^n$ crece de manera exponencial. Cuando $|\\lambda| < 1$, el tamaño de la desviación se vuelve exponencialmente pequeño.\n",
    "\n",
    "En ambos casos, podemos tomar medidas para controlar el comportamiento del gradiente a medida que $n$ aumenta:\n",
    "\n",
    "- **Para $|\\lambda_i| < 1$:** Si queremos evitar que el gradiente se vuelva demasiado pequeño, podemos utilizar técnicas como el recorte de gradientes (gradient clipping) para limitar la magnitud del gradiente durante el entrenamiento.\n",
    "\n",
    "- **Para $|\\lambda_i| > 1$:** Si el gradiente crece indefinidamente y provoca inestabilidad en el entrenamiento, podemos utilizar técnicas como la normalización de lotes (batch normalization) o la inicialización de pesos adecuada para estabilizar el entrenamiento y evitar que el gradiente se vuelva explosivo.\n",
    "\n",
    "En ambos casos, el comportamiento del gradiente está relacionado con los valores propios de la matriz $V$, y comprender estos valores propios es fundamental para gestionar el entrenamiento de redes neuronales recurrentes (RNN) de manera efectiva.\n",
    "\n",
    "Además en la bibliografía se menciona como con un mapa nonlinear el Jacobiano es libre de cambiar en cada iteración, y esto puede ayudar a controlar el gradiente y sus valores propios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Escriba las ecuaciones de una celda Long Short-Term Memory (LSTM) y dibuje su grafo (sin desenrollar). Para ello, considere la versión tradicional sin \"peepholes connections\". \n",
    "\n",
    "\n",
    "Explique cada una de las compuertas de la celda ¿Qué ocurre cuando cada una se abre o cierra? ¿Cómo soluciona la celda LSTM los problemas con la norma del gradiente observados en las preguntas anteriores? \n",
    "\n",
    "Sea $c_t$ la memoria de la celda en el tiempo $t$ y considere solo el camino lineal que conecta $c_{t-1}$ con $c_t$. Se sabe que es conveniente inicializar el sesgo de la compuerta de olvido en 1 en lugar de 0 como es usual para otras capas neuronales ¿Por qué?\n",
    "\n",
    "Respuesta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las ecuaciones de una celda Long Short-Term Memory (LSTM) son las siguientes:\n",
    "\n",
    "- **Celda de memoria**: $c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$\n",
    "- **Salida de la celda**: $h_t = o_t \\odot \\tanh(c_t)$\n",
    "- **Compuerta de olvido (Forget gate)**: $f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$\n",
    "- **Compuerta de entrada (Input gate)**: $i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$\n",
    "- **Compuerta de salida (Output gate)**: $o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$\n",
    "- **Candidato a celda de memoria**: $\\tilde{c}_t = \\tanh(W_c \\cdot [h_{t-1}, x_t] + b_c)$\n",
    "\n",
    "Donde:\n",
    "- $\\sigma$ es la función sigmoide.\n",
    "- $\\odot$ denota la multiplicación elemento por elemento.\n",
    "- $h_t$ es la salida en el tiempo $t$.\n",
    "- $c_t$ es la memoria de la celda en el tiempo $t$.\n",
    "- $f_t$ es la compuerta de olvido en el tiempo $t$, que controla cuánta información de la memoria anterior se descarta.\n",
    "- $i_t$ es la compuerta de entrada en el tiempo $t$, que controla cuánta información nueva se agrega a la memoria.\n",
    "- $o_t$ es la compuerta de salida en el tiempo $t$, que controla cuánta memoria se expone como salida.\n",
    "\n",
    "<img src=\"images/LSTM.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "Cuando la compuerta de olvido se abre (cerca de 1), la celda LSTM retiene más información de la memoria anterior. Cuando la compuerta de entrada se abre (cerca de 1), se permite que nueva información influya en la memoria de la celda. La compuerta de salida controla cuánta memoria se muestra como salida.\n",
    "\n",
    "La celda LSTM resuelve los problemas con la norma del gradiente mediante el uso de compuertas que regulan el flujo de información. Esto permite que el gradiente fluya de manera más estable durante el entrenamiento, evitando el problema de desvanecimiento o explosión de gradientes.\n",
    "\n",
    "Es conveniente inicializar el sesgo de la compuerta de olvido en 1 en lugar de 0 porque esto permite que la celda LSTM comience por defecto manteniendo su memoria anterior. Esto es útil para problemas donde es importante retener información a lo largo del tiempo, ya que la compuerta de olvido controlará qué parte de la memoria anterior se mantiene."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "EL7006_T1_RNN_P1_Enunciado.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
